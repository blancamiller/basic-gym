{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "The purpose of this tutorial is to walk-through how to implement Value Iteration, a dynamic programming method. We will use frozenlake, an existing environment developed by openai gym. This tutotial will be broken up into four parts (see below). We will also walk-through the deterministic and stochastic cases and a few discount factors to gain intuition for how this algorithm works.  \n",
    "\n",
    "### 4 Parts:\n",
    "\n",
    "- Policy Evaluation (prediction)\n",
    "- Policy Improvement\n",
    "- Policy Iteration \n",
    "- Value Iteration\n",
    "\n",
    "Reference: Sutton & Barto, 2018, Reinforcement Learning: An Introduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment \n",
    "\n",
    "Reference: https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "A 4x4 gridworld with several states: \n",
    "\n",
    "- S: starting point, safe\n",
    "- F: frozen surface, safe\n",
    "- H: hole, fall to your doom\n",
    "- G: goal, where the frisbee is located\n",
    "\n",
    "When you run the environment, the gridworld will be represented as: \n",
    "\n",
    "SFFF       \n",
    "FHFH       \n",
    "FFFH       \n",
    "HFFG  \n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from lake_envs import *\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering Function -- Do NOT need to modify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_single(env, policy, max_steps=100):\n",
    "  \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "  \"\"\"\n",
    "\n",
    "  episode_reward = 0\n",
    "  ob = env.reset()\n",
    "  for t in range(max_steps):\n",
    "    env.render()\n",
    "    time.sleep(0.25)\n",
    "    a = policy[ob]\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    episode_reward += rew\n",
    "    if done:\n",
    "      break\n",
    "  env.render();\n",
    "  if not done:\n",
    "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "  else:\n",
    "    print('# Max Steps: ',max_steps)\n",
    "    print(\"Episode reward: %f\" % episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine & Initialize Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability, nextstate, reward, terminal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 0, 0.0, False)],\n",
       " 1: [(1.0, 4, 0.0, False)],\n",
       " 2: [(1.0, 1, 0.0, False)],\n",
       " 3: [(1.0, 0, 0.0, False)]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the deterministic environment\n",
    "env_d = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "print('probability, nextstate, reward, terminal')\n",
    "env_d.P[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability, nextstate, reward, terminal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False)],\n",
       " 2: [(0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the stochastic environment\n",
    "env_s = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "print('probability, nextstate, reward, terminal')\n",
    "env_s.P[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Understanding\n",
    "\n",
    "Notice that the deterministic environment has 1 cell per parameter, but the stochastic environment has 3 cells. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation (prediction): \n",
    "\n",
    "The process for evaluating a policy is to successively approximate and update the value of a state using the Bellman equation. The old state's value is replaced with a new value. The new value is obtained by using the old value of the successor state, s', and the rewards we expect to get at the next state. Then, the value function is computer by summing the expectations for all of the successeeding next states.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm: Iterative Policy Evaluation for estimating V~v_pi [p. 75]\n",
    "\n",
    "Input pi, the policy we are evaluating\n",
    "\n",
    "Set theta > 0, a small threshold that will determine the accuracy of our policy's estimation\n",
    "\n",
    "Initialize V(s) arbitrarily, the initial state values, for all states except the terminal state set to zero \n",
    "\n",
    "Loop:\n",
    "\n",
    "    delta <- 0\n",
    "    Loop for each state in S:\n",
    "        v <- V(s)\n",
    "        V(s) <- sum over all actions, pi(a|s) \n",
    "                * sum over all next states and their corresponding \n",
    "                rewards, p(s',r|s,a)[r + gamma * V(s')]\n",
    "        delta <- max(delta, |v - V(s)|) \n",
    "    until delta < theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=1, tol=1e-3):\n",
    "    \n",
    "    # Initialize value function & delta\n",
    "    V = np.zeros(nS)\n",
    "    print('Initial Value Function',V)\n",
    "    delta = np.inf\n",
    "    episode = 0\n",
    "    \n",
    "    # Policy eval. will terminate when the value function's change is below the threshold\n",
    "    while episode < 10:\n",
    "    #while delta >= tol:\n",
    "        \n",
    "        print('Episode:',episode, ' &  Value Function',V)\n",
    "        \n",
    "        # Why do we loop through all the states? \n",
    "        for s in range(nS):   \n",
    "            v = V[s]\n",
    "            a = policy[s]\n",
    "            \n",
    "            for prob, nextstate, reward, done in P[s][a]:   \n",
    "                V[s] = prob * (reward + gamma * nextstate) \n",
    "                \n",
    "                #print('state:',s)\n",
    "                #print('action: ',a)\n",
    "                #print('prob: ',prob)\n",
    "                #print('nextstate: ',nextstate)\n",
    "                #print('reward: ',reward)\n",
    "                #print('done: ',done) \n",
    "                #print('value: ',V[s])\n",
    "                      \n",
    "                # Compute the change in value functions across states\n",
    "                delta = max(delta, np.abs(v - V[s]))\n",
    "\n",
    "        episode+=1\n",
    "        #if episode < 10:\n",
    "            #print('Episode: ',episode)\n",
    "            #print('Working Value Fcn: ',V)\n",
    "        \n",
    "        \"\"\"\n",
    "        if episode % 10 == 0:\n",
    "            print('Episode: ',episode)\n",
    "            print('value_function: ',V)\n",
    "        \"\"\"\n",
    "        \n",
    "    # Final value function\n",
    "    print('Final # of Episodes: ',episode)\n",
    "    value_function = np.array(V)\n",
    "        \n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Deterministic Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine a Deterministic Policies for a Discount Value of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Deterministic Zeros Policy\n",
    "policy = np.zeros(env_d.nS, dtype=int)\n",
    "print('Policy: ',policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Zero Policy Iteration\n",
      "-------------------------------\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 1  &  Value Function [ 0.  0.  1.  2.  4.  5.  5.  7.  8.  8.  9. 11. 12. 12. 13. 15.]\n",
      "Episode: 2  &  Value Function [ 0.  0.  1.  2.  4.  5.  5.  7.  8.  8.  9. 11. 12. 12. 13. 15.]\n",
      "Episode: 3  &  Value Function [ 0.  0.  1.  2.  4.  5.  5.  7.  8.  8.  9. 11. 12. 12. 13. 15.]\n",
      "Episode: 4  &  Value Function [ 0.  0.  1.  2.  4.  5.  5.  7.  8.  8.  9. 11. 12. 12. 13. 15.]\n",
      "Episode: 5  &  Value Function [ 0.  0.  1.  2.  4.  5.  5.  7.  8.  8.  9. 11. 12. 12. 13. 15.]\n",
      "Episode: 6  &  Value Function [ 0.  0.  1.  2.  4.  5.  5.  7.  8.  8.  9. 11. 12. 12. 13. 15.]\n",
      "Episode: 7  &  Value Function [ 0.  0.  1.  2.  4.  5.  5.  7.  8.  8.  9. 11. 12. 12. 13. 15.]\n",
      "Episode: 8  &  Value Function [ 0.  0.  1.  2.  4.  5.  5.  7.  8.  8.  9. 11. 12. 12. 13. 15.]\n",
      "Episode: 9  &  Value Function [ 0.  0.  1.  2.  4.  5.  5.  7.  8.  8.  9. 11. 12. 12. 13. 15.]\n",
      "Final # of Episodes:  10\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a Deterministic Zeros Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Zero Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=1, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Values:  [ 0.  0.  1.  2.  4.  5.  5.  7.  8.  8.  9. 11. 12. 12. 13. 15.]\n"
     ]
    }
   ],
   "source": [
    "# Examine a Deterministic Zeros Policy & Values\n",
    "print('Policy: ',policy)\n",
    "print('Values: ',state_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Understanding -- Zero Policy\n",
    "\n",
    "Does this value function make sense to you? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 3 steps.\n"
     ]
    }
   ],
   "source": [
    "# Inspect Behavior for a Deterministic Zeros Policy\n",
    "render_single(env_d, policy, max_steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Understanding -- Zero Policy\n",
    "\n",
    "Our agent doesn't reach a terminal state in 5 steps. It turns out that if we instead tried for 100 steps, our agent still wouldn't reach a terminal state. In an environment of only 16 states and 64 actions, it seems like we should reach a terminal states, so why don't we? (Hint: Observe the behavior of the highlighted state.)\n",
    "\n",
    "If the values of our value function didn't make sense to you before, do they now? What new realization did you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Ones Policy Iteration\n",
      "-------------------------------\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 1  &  Value Function [ 4.  5.  6.  7.  8.  5. 10.  7. 12. 13. 14. 11. 12. 13. 14. 15.]\n",
      "Episode: 2  &  Value Function [ 4.  5.  6.  7.  8.  5. 10.  7. 12. 13. 14. 11. 12. 13. 14. 15.]\n",
      "Episode: 3  &  Value Function [ 4.  5.  6.  7.  8.  5. 10.  7. 12. 13. 14. 11. 12. 13. 14. 15.]\n",
      "Episode: 4  &  Value Function [ 4.  5.  6.  7.  8.  5. 10.  7. 12. 13. 14. 11. 12. 13. 14. 15.]\n",
      "Episode: 5  &  Value Function [ 4.  5.  6.  7.  8.  5. 10.  7. 12. 13. 14. 11. 12. 13. 14. 15.]\n",
      "Episode: 6  &  Value Function [ 4.  5.  6.  7.  8.  5. 10.  7. 12. 13. 14. 11. 12. 13. 14. 15.]\n",
      "Episode: 7  &  Value Function [ 4.  5.  6.  7.  8.  5. 10.  7. 12. 13. 14. 11. 12. 13. 14. 15.]\n",
      "Episode: 8  &  Value Function [ 4.  5.  6.  7.  8.  5. 10.  7. 12. 13. 14. 11. 12. 13. 14. 15.]\n",
      "Episode: 9  &  Value Function [ 4.  5.  6.  7.  8.  5. 10.  7. 12. 13. 14. 11. 12. 13. 14. 15.]\n",
      "Final # of Episodes:  10\n",
      "\n",
      "Policy:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Final Values:  [ 4.  5.  6.  7.  8.  5. 10.  7. 12. 13. 14. 11. 12. 13. 14. 15.]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "# Max Steps:  3\n",
      "Episode reward: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Ones Policy\n",
    "policy = np.ones(env_d.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Ones Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Ones Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=1, tol=1e-3)\n",
    "\n",
    "# Examine a Ones Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Final Values: ',state_values)\n",
    "\n",
    "# Inspect the Behavior of a Ones Policy\n",
    "render_single(env_d, policy, max_steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Understanding -- Ones Policy\n",
    "\n",
    "This policy does reach a terminal state after only 5 steps. Additionally, there is a difference in behavior with \n",
    "a policy of ones. What is that behavior? What does this mean? (Hint: If you're unsure test for policies of all twos, threes, fours and so on until you've figured it out.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Twos Policy Iteration\n",
      "-------------------------------\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 1  &  Value Function [ 1.  2.  3.  3.  5.  5.  7.  7.  9. 10. 11. 11. 12. 14. 16. 15.]\n",
      "Episode: 2  &  Value Function [ 1.  2.  3.  3.  5.  5.  7.  7.  9. 10. 11. 11. 12. 14. 16. 15.]\n",
      "Episode: 3  &  Value Function [ 1.  2.  3.  3.  5.  5.  7.  7.  9. 10. 11. 11. 12. 14. 16. 15.]\n",
      "Episode: 4  &  Value Function [ 1.  2.  3.  3.  5.  5.  7.  7.  9. 10. 11. 11. 12. 14. 16. 15.]\n",
      "Episode: 5  &  Value Function [ 1.  2.  3.  3.  5.  5.  7.  7.  9. 10. 11. 11. 12. 14. 16. 15.]\n",
      "Episode: 6  &  Value Function [ 1.  2.  3.  3.  5.  5.  7.  7.  9. 10. 11. 11. 12. 14. 16. 15.]\n",
      "Episode: 7  &  Value Function [ 1.  2.  3.  3.  5.  5.  7.  7.  9. 10. 11. 11. 12. 14. 16. 15.]\n",
      "Episode: 8  &  Value Function [ 1.  2.  3.  3.  5.  5.  7.  7.  9. 10. 11. 11. 12. 14. 16. 15.]\n",
      "Episode: 9  &  Value Function [ 1.  2.  3.  3.  5.  5.  7.  7.  9. 10. 11. 11. 12. 14. 16. 15.]\n",
      "Final # of Episodes:  10\n",
      "\n",
      "Policy:  [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Final Values:  [ 1.  2.  3.  3.  5.  5.  7.  7.  9. 10. 11. 11. 12. 14. 16. 15.]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 3 steps.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Twos Policy\n",
    "policy = np.array([2]*env_d.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Twos Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Twos Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=1, tol=1e-3)\n",
    "\n",
    "# Examine a Twos Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Final Values: ',state_values)\n",
    "\n",
    "# Inspect the Behavior of a Twos Policy\n",
    "render_single(env_d, policy, max_steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Understanding -- Twos Policy\n",
    "\n",
    "Describe the agent's behavior. Does it make sense that the agent didn't reach a terminal state? Why? Do the values of our value function make sense? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------\n",
      "Beginning Threes Policy Iteration\n",
      "---------------------------------\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 1  &  Value Function [ 0.  1.  2.  3.  0.  5.  2.  7.  4.  5.  6. 11. 12.  9. 10. 15.]\n",
      "Episode: 2  &  Value Function [ 0.  1.  2.  3.  0.  5.  2.  7.  4.  5.  6. 11. 12.  9. 10. 15.]\n",
      "Episode: 3  &  Value Function [ 0.  1.  2.  3.  0.  5.  2.  7.  4.  5.  6. 11. 12.  9. 10. 15.]\n",
      "Episode: 4  &  Value Function [ 0.  1.  2.  3.  0.  5.  2.  7.  4.  5.  6. 11. 12.  9. 10. 15.]\n",
      "Episode: 5  &  Value Function [ 0.  1.  2.  3.  0.  5.  2.  7.  4.  5.  6. 11. 12.  9. 10. 15.]\n",
      "Episode: 6  &  Value Function [ 0.  1.  2.  3.  0.  5.  2.  7.  4.  5.  6. 11. 12.  9. 10. 15.]\n",
      "Episode: 7  &  Value Function [ 0.  1.  2.  3.  0.  5.  2.  7.  4.  5.  6. 11. 12.  9. 10. 15.]\n",
      "Episode: 8  &  Value Function [ 0.  1.  2.  3.  0.  5.  2.  7.  4.  5.  6. 11. 12.  9. 10. 15.]\n",
      "Episode: 9  &  Value Function [ 0.  1.  2.  3.  0.  5.  2.  7.  4.  5.  6. 11. 12.  9. 10. 15.]\n",
      "Final # of Episodes:  10\n",
      "\n",
      "Policy:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "Final Values:  [ 0.  1.  2.  3.  0.  5.  2.  7.  4.  5.  6. 11. 12.  9. 10. 15.]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 3 steps.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Threes Policy\n",
    "policy = np.array([3]*env_d.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Threes Policy \n",
    "print(\"\\n\" + \"-\"*33 + \"\\nBeginning Threes Policy Iteration\\n\" + \"-\"*33)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=1, tol=1e-3)\n",
    "\n",
    "# Examine a Threes Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Final Values: ',state_values)\n",
    "\n",
    "# Inspect the Behavior of a ThreesPolicy\n",
    "render_single(env_d, policy, max_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------\n",
      "Beginning Fours Policy Iteration\n",
      "---------------------------------\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-804a29a183e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Evaluate a Fours Policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m33\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\nBeginning Fours Policy Iteration\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mstate_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-59ad667b45db>\u001b[0m in \u001b[0;36mpolicy_evaluation\u001b[0;34m(P, nS, nA, policy, gamma, tol)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnextstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnextstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 4"
     ]
    }
   ],
   "source": [
    "# Initialize a Fours Policy\n",
    "policy = np.array([4]*env_d.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Fours Policy \n",
    "print(\"\\n\" + \"-\"*33 + \"\\nBeginning Fours Policy Iteration\\n\" + \"-\"*33)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=1, tol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Understanding? \n",
    "\n",
    "Why do we get an error when we try to implement a Fours policy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Deterministic Policies for a Discount Value of 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Zero Policy Iteration\n",
      "-------------------------------\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 1  &  Value Function [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode: 2  &  Value Function [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode: 3  &  Value Function [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode: 4  &  Value Function [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode: 5  &  Value Function [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode: 6  &  Value Function [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode: 7  &  Value Function [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode: 8  &  Value Function [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode: 9  &  Value Function [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Final # of Episodes:  10\n",
      "\n",
      "Policy:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Final Values:  [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 3 steps.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Deterministic Zeros Policy\n",
    "policy = np.zeros(env_d.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Deterministic Zeros Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Zero Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=0.9, tol=1e-3)\n",
    "\n",
    "# Examine a Deterministic Zeros Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Final Values: ',state_values)\n",
    "\n",
    "# Inspect Behavior for a Deterministic Zeros Policy\n",
    "render_single(env_d, policy, max_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Ones Policy Iteration\n",
      "-------------------------------\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 1  &  Value Function [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode: 2  &  Value Function [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode: 3  &  Value Function [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode: 4  &  Value Function [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode: 5  &  Value Function [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode: 6  &  Value Function [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode: 7  &  Value Function [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode: 8  &  Value Function [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode: 9  &  Value Function [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Final # of Episodes:  10\n",
      "\n",
      "Policy:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Values:  [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "# Max Steps:  3\n",
      "Episode reward: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Deterministic Ones Policy\n",
    "policy = np.ones(env_d.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Deterministic Ones Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Ones Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=0.9, tol=1e-3)\n",
    "\n",
    "# Examine a Deterministic Ones Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Values: ',state_values)\n",
    "\n",
    "# Inspect Behavior for a Deterministic Ones Policy\n",
    "render_single(env_d, policy, max_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Twos Policy Iteration\n",
      "-------------------------------\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 1  &  Value Function [ 0.9  1.8  2.7  2.7  4.5  4.5  6.3  6.3  8.1  9.   9.9  9.9 10.8 12.6\n",
      " 14.5 13.5]\n",
      "Episode: 2  &  Value Function [ 0.9  1.8  2.7  2.7  4.5  4.5  6.3  6.3  8.1  9.   9.9  9.9 10.8 12.6\n",
      " 14.5 13.5]\n",
      "Episode: 3  &  Value Function [ 0.9  1.8  2.7  2.7  4.5  4.5  6.3  6.3  8.1  9.   9.9  9.9 10.8 12.6\n",
      " 14.5 13.5]\n",
      "Episode: 4  &  Value Function [ 0.9  1.8  2.7  2.7  4.5  4.5  6.3  6.3  8.1  9.   9.9  9.9 10.8 12.6\n",
      " 14.5 13.5]\n",
      "Episode: 5  &  Value Function [ 0.9  1.8  2.7  2.7  4.5  4.5  6.3  6.3  8.1  9.   9.9  9.9 10.8 12.6\n",
      " 14.5 13.5]\n",
      "Episode: 6  &  Value Function [ 0.9  1.8  2.7  2.7  4.5  4.5  6.3  6.3  8.1  9.   9.9  9.9 10.8 12.6\n",
      " 14.5 13.5]\n",
      "Episode: 7  &  Value Function [ 0.9  1.8  2.7  2.7  4.5  4.5  6.3  6.3  8.1  9.   9.9  9.9 10.8 12.6\n",
      " 14.5 13.5]\n",
      "Episode: 8  &  Value Function [ 0.9  1.8  2.7  2.7  4.5  4.5  6.3  6.3  8.1  9.   9.9  9.9 10.8 12.6\n",
      " 14.5 13.5]\n",
      "Episode: 9  &  Value Function [ 0.9  1.8  2.7  2.7  4.5  4.5  6.3  6.3  8.1  9.   9.9  9.9 10.8 12.6\n",
      " 14.5 13.5]\n",
      "Final # of Episodes:  10\n",
      "\n",
      "Policy:  [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Values:  [ 0.9  1.8  2.7  2.7  4.5  4.5  6.3  6.3  8.1  9.   9.9  9.9 10.8 12.6\n",
      " 14.5 13.5]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 10 steps.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Deterministic Twos Policy\n",
    "policy = np.array([2]*env_d.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Determinstic Twos Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Twos Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=0.9, tol=1e-3)\n",
    "\n",
    "# Examine a Deterministic Twos Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Values: ',state_values)\n",
    "\n",
    "# Inspect Behavior for a Deterministic Twos Policy\n",
    "render_single(env_d, policy, max_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding \n",
    "\n",
    "Why don't we reach a terminal state in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Twos Policy Iteration\n",
      "-------------------------------\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 1  &  Value Function [ 0.   0.9  1.8  2.7  0.   4.5  1.8  6.3  3.6  4.5  5.4  9.9 10.8  8.1\n",
      "  9.  13.5]\n",
      "Episode: 2  &  Value Function [ 0.   0.9  1.8  2.7  0.   4.5  1.8  6.3  3.6  4.5  5.4  9.9 10.8  8.1\n",
      "  9.  13.5]\n",
      "Episode: 3  &  Value Function [ 0.   0.9  1.8  2.7  0.   4.5  1.8  6.3  3.6  4.5  5.4  9.9 10.8  8.1\n",
      "  9.  13.5]\n",
      "Episode: 4  &  Value Function [ 0.   0.9  1.8  2.7  0.   4.5  1.8  6.3  3.6  4.5  5.4  9.9 10.8  8.1\n",
      "  9.  13.5]\n",
      "Episode: 5  &  Value Function [ 0.   0.9  1.8  2.7  0.   4.5  1.8  6.3  3.6  4.5  5.4  9.9 10.8  8.1\n",
      "  9.  13.5]\n",
      "Episode: 6  &  Value Function [ 0.   0.9  1.8  2.7  0.   4.5  1.8  6.3  3.6  4.5  5.4  9.9 10.8  8.1\n",
      "  9.  13.5]\n",
      "Episode: 7  &  Value Function [ 0.   0.9  1.8  2.7  0.   4.5  1.8  6.3  3.6  4.5  5.4  9.9 10.8  8.1\n",
      "  9.  13.5]\n",
      "Episode: 8  &  Value Function [ 0.   0.9  1.8  2.7  0.   4.5  1.8  6.3  3.6  4.5  5.4  9.9 10.8  8.1\n",
      "  9.  13.5]\n",
      "Episode: 9  &  Value Function [ 0.   0.9  1.8  2.7  0.   4.5  1.8  6.3  3.6  4.5  5.4  9.9 10.8  8.1\n",
      "  9.  13.5]\n",
      "Final # of Episodes:  10\n",
      "\n",
      "Policy:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "Final Values:  [ 0.   0.9  1.8  2.7  0.   4.5  1.8  6.3  3.6  4.5  5.4  9.9 10.8  8.1\n",
      "  9.  13.5]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 3 steps.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Deterministic Threes Policy\n",
    "policy = np.array([3]*env_d.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Deterministic Threes Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Twos Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=0.9, tol=1e-3)\n",
    "\n",
    "# Examine a Threes Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Final Values: ',state_values)\n",
    "\n",
    "# Inspect Behavior for a Threes Policy\n",
    "render_single(env_d, policy, max_steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Stochastic Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Stochastic Policies for Discount Value of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall we initialized a stochastic environment\n",
    "env_s = gym.make(\"Stochastic-4x4-FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Zeros Policy Iteration\n",
      "-------------------------------\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 1  &  Value Function [ 1.333  1.667  2.     2.333  2.667  5.     3.333  7.     4.     4.333\n",
      "  4.667 11.    12.     4.333  4.667 15.   ]\n",
      "Episode: 2  &  Value Function [ 1.333  1.667  2.     2.333  2.667  5.     3.333  7.     4.     4.333\n",
      "  4.667 11.    12.     4.333  4.667 15.   ]\n",
      "Episode: 3  &  Value Function [ 1.333  1.667  2.     2.333  2.667  5.     3.333  7.     4.     4.333\n",
      "  4.667 11.    12.     4.333  4.667 15.   ]\n",
      "Episode: 4  &  Value Function [ 1.333  1.667  2.     2.333  2.667  5.     3.333  7.     4.     4.333\n",
      "  4.667 11.    12.     4.333  4.667 15.   ]\n",
      "Episode: 5  &  Value Function [ 1.333  1.667  2.     2.333  2.667  5.     3.333  7.     4.     4.333\n",
      "  4.667 11.    12.     4.333  4.667 15.   ]\n",
      "Episode: 6  &  Value Function [ 1.333  1.667  2.     2.333  2.667  5.     3.333  7.     4.     4.333\n",
      "  4.667 11.    12.     4.333  4.667 15.   ]\n",
      "Episode: 7  &  Value Function [ 1.333  1.667  2.     2.333  2.667  5.     3.333  7.     4.     4.333\n",
      "  4.667 11.    12.     4.333  4.667 15.   ]\n",
      "Episode: 8  &  Value Function [ 1.333  1.667  2.     2.333  2.667  5.     3.333  7.     4.     4.333\n",
      "  4.667 11.    12.     4.333  4.667 15.   ]\n",
      "Episode: 9  &  Value Function [ 1.333  1.667  2.     2.333  2.667  5.     3.333  7.     4.     4.333\n",
      "  4.667 11.    12.     4.333  4.667 15.   ]\n",
      "Final # of Episodes:  10\n",
      "\n",
      "Policy:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Values:  [ 1.333  1.667  2.     2.333  2.667  5.     3.333  7.     4.     4.333\n",
      "  4.667 11.    12.     4.333  4.667 15.   ]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 5 steps.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Stochastic Zeros Policy\n",
    "policy = np.zeros(env_s.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Stochastic Zeros Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Zeros Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_s.P, env_s.nS, env_s.nA, policy, gamma=1, tol=1e-3)\n",
    "\n",
    "# Examine a Stochastic Zeros Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Values: ',state_values)\n",
    "\n",
    "# Inspect Behavior for a Stochastic Zeros Policy\n",
    "render_single(env_s, policy, max_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Ones Policy Iteration\n",
      "-------------------------------\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 1  &  Value Function [ 0.333  0.667  1.     1.     1.667  5.     2.333  7.     3.     3.333\n",
      "  3.667 11.    12.     4.667  5.333 15.   ]\n",
      "Episode: 2  &  Value Function [ 0.333  0.667  1.     1.     1.667  5.     2.333  7.     3.     3.333\n",
      "  3.667 11.    12.     4.667  5.333 15.   ]\n",
      "Episode: 3  &  Value Function [ 0.333  0.667  1.     1.     1.667  5.     2.333  7.     3.     3.333\n",
      "  3.667 11.    12.     4.667  5.333 15.   ]\n",
      "Episode: 4  &  Value Function [ 0.333  0.667  1.     1.     1.667  5.     2.333  7.     3.     3.333\n",
      "  3.667 11.    12.     4.667  5.333 15.   ]\n",
      "Episode: 5  &  Value Function [ 0.333  0.667  1.     1.     1.667  5.     2.333  7.     3.     3.333\n",
      "  3.667 11.    12.     4.667  5.333 15.   ]\n",
      "Episode: 6  &  Value Function [ 0.333  0.667  1.     1.     1.667  5.     2.333  7.     3.     3.333\n",
      "  3.667 11.    12.     4.667  5.333 15.   ]\n",
      "Episode: 7  &  Value Function [ 0.333  0.667  1.     1.     1.667  5.     2.333  7.     3.     3.333\n",
      "  3.667 11.    12.     4.667  5.333 15.   ]\n",
      "Episode: 8  &  Value Function [ 0.333  0.667  1.     1.     1.667  5.     2.333  7.     3.     3.333\n",
      "  3.667 11.    12.     4.667  5.333 15.   ]\n",
      "Episode: 9  &  Value Function [ 0.333  0.667  1.     1.     1.667  5.     2.333  7.     3.     3.333\n",
      "  3.667 11.    12.     4.667  5.333 15.   ]\n",
      "Final # of Episodes:  10\n",
      "\n",
      "Policy:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Values:  [ 0.333  0.667  1.     1.     1.667  5.     2.333  7.     3.     3.333\n",
      "  3.667 11.    12.     4.667  5.333 15.   ]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "# Max Steps:  3\n",
      "Episode reward: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Stochastic Ones Policy\n",
    "policy = np.ones(env_s.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Stochastic Ones Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Ones Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_s.P, env_s.nS, env_s.nA, policy, gamma=1, tol=1e-3)\n",
    "\n",
    "# Examine a Stochastic Ones Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Values: ',state_values)\n",
    "\n",
    "# Inspect Behavior for a Stochastic Ones Policy\n",
    "render_single(env_s, policy, max_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Twos Policy Iteration\n",
      "-------------------------------\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 1  &  Value Function [ 0.     0.333  0.667  1.     0.     5.     0.667  7.     1.333  1.667\n",
      "  2.    11.    12.     3.     3.333 15.   ]\n",
      "Episode: 2  &  Value Function [ 0.     0.333  0.667  1.     0.     5.     0.667  7.     1.333  1.667\n",
      "  2.    11.    12.     3.     3.333 15.   ]\n",
      "Episode: 3  &  Value Function [ 0.     0.333  0.667  1.     0.     5.     0.667  7.     1.333  1.667\n",
      "  2.    11.    12.     3.     3.333 15.   ]\n",
      "Episode: 4  &  Value Function [ 0.     0.333  0.667  1.     0.     5.     0.667  7.     1.333  1.667\n",
      "  2.    11.    12.     3.     3.333 15.   ]\n",
      "Episode: 5  &  Value Function [ 0.     0.333  0.667  1.     0.     5.     0.667  7.     1.333  1.667\n",
      "  2.    11.    12.     3.     3.333 15.   ]\n",
      "Episode: 6  &  Value Function [ 0.     0.333  0.667  1.     0.     5.     0.667  7.     1.333  1.667\n",
      "  2.    11.    12.     3.     3.333 15.   ]\n",
      "Episode: 7  &  Value Function [ 0.     0.333  0.667  1.     0.     5.     0.667  7.     1.333  1.667\n",
      "  2.    11.    12.     3.     3.333 15.   ]\n",
      "Episode: 8  &  Value Function [ 0.     0.333  0.667  1.     0.     5.     0.667  7.     1.333  1.667\n",
      "  2.    11.    12.     3.     3.333 15.   ]\n",
      "Episode: 9  &  Value Function [ 0.     0.333  0.667  1.     0.     5.     0.667  7.     1.333  1.667\n",
      "  2.    11.    12.     3.     3.333 15.   ]\n",
      "Final # of Episodes:  10\n",
      "\n",
      "Policy:  [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Values:  [ 0.     0.333  0.667  1.     0.     5.     0.667  7.     1.333  1.667\n",
      "  2.    11.    12.     3.     3.333 15.   ]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 3 steps.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Stochastic Twos Policy\n",
    "policy = np.array([2]*env_s.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Stochastic Twos Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Twos Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_s.P, env_s.nS, env_s.nA, policy, gamma=1, tol=1e-3)\n",
    "\n",
    "# Examine a Stochastic Twos Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Values: ',state_values)\n",
    "\n",
    "# Inspect Behavior for a Stochastic Twos Policy\n",
    "render_single(env_s, policy, max_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Threes Policy Iteration\n",
      "-------------------------------\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode: 1  &  Value Function [ 0.     0.     0.333  0.667  1.333  5.     1.667  7.     2.667  2.667\n",
      "  3.    11.    12.     4.     4.333 15.   ]\n",
      "Episode: 2  &  Value Function [ 0.     0.     0.333  0.667  1.333  5.     1.667  7.     2.667  2.667\n",
      "  3.    11.    12.     4.     4.333 15.   ]\n",
      "Episode: 3  &  Value Function [ 0.     0.     0.333  0.667  1.333  5.     1.667  7.     2.667  2.667\n",
      "  3.    11.    12.     4.     4.333 15.   ]\n",
      "Episode: 4  &  Value Function [ 0.     0.     0.333  0.667  1.333  5.     1.667  7.     2.667  2.667\n",
      "  3.    11.    12.     4.     4.333 15.   ]\n",
      "Episode: 5  &  Value Function [ 0.     0.     0.333  0.667  1.333  5.     1.667  7.     2.667  2.667\n",
      "  3.    11.    12.     4.     4.333 15.   ]\n",
      "Episode: 6  &  Value Function [ 0.     0.     0.333  0.667  1.333  5.     1.667  7.     2.667  2.667\n",
      "  3.    11.    12.     4.     4.333 15.   ]\n",
      "Episode: 7  &  Value Function [ 0.     0.     0.333  0.667  1.333  5.     1.667  7.     2.667  2.667\n",
      "  3.    11.    12.     4.     4.333 15.   ]\n",
      "Episode: 8  &  Value Function [ 0.     0.     0.333  0.667  1.333  5.     1.667  7.     2.667  2.667\n",
      "  3.    11.    12.     4.     4.333 15.   ]\n",
      "Episode: 9  &  Value Function [ 0.     0.     0.333  0.667  1.333  5.     1.667  7.     2.667  2.667\n",
      "  3.    11.    12.     4.     4.333 15.   ]\n",
      "Final # of Episodes:  10\n",
      "\n",
      "Policy:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "Values:  [ 0.     0.     0.333  0.667  1.333  5.     1.667  7.     2.667  2.667\n",
      "  3.    11.    12.     4.     4.333 15.   ]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 3 steps.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Stochastic Threes Policy\n",
    "policy = np.array([3]*env_s.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Stochastic Threes Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Threes Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_s.P, env_s.nS, env_s.nA, policy, gamma=1, tol=1e-3)\n",
    "\n",
    "# Examine a Stochastic Threes Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Values: ',state_values)\n",
    "\n",
    "# Inspect Behavior for a Stochastic Threes Policy\n",
    "render_single(env_s, policy, max_steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration (using iterative policy evaluation) for estimating pi ~= pi*\n",
    "\n",
    "1. Initialization \n",
    "V(s) is an element of the real numbers for all states\n",
    "pi(s) is an element of A(s) for all states \n",
    "\n",
    "2. Policy Evaluation (implemented above)\n",
    "Loop: \n",
    "    delta <- 0\n",
    "    Loop for each s in the set of States:\n",
    "        v <- V(s)\n",
    "        V(s) <- sum over all next states & rewards p(s',r|s,a) * [r + gamma * V(s')]\n",
    "        delta <- max(delta, |v-V(s)|)\n",
    "    until delta < theta \n",
    "    \n",
    "3. Policy Improvement (still need to implement)\n",
    "policy-stable <- true\n",
    "For each s in the set of States:\n",
    "    old-action <- pi(s)\n",
    "    pi(s) <- argmax over actions sum over all next states and rewards, p(s',r|s,a)[r + gamma * V(s)]\n",
    "    If old-action =/= pi()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                    # ------------ Deviation from 4.1 algorithm ------------ #\\n                    # Loop through set of possible next actions\\n                    \\n                    for a, action_prob in enumerate(policy[s]):\\n                        # For each action, look at its possible next state\\n                        for prob, next_state, reward, done in P[s][a]:\\n\\n                            # Calculate the expected value using equation 4.6\\n                            v += action_prob * prob * (reward + gamma * ) \\n                            \\n                    # ------------ Deviation from 4.1 algorithm ------------ #\\n                    '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Policy Evaluation \n",
    "\"\"\"\n",
    "                    # ------------ Deviation from 4.1 algorithm ------------ #\n",
    "                    # Loop through set of possible next actions\n",
    "                    \n",
    "                    for a, action_prob in enumerate(policy[s]):\n",
    "                        # For each action, look at its possible next state\n",
    "                        for prob, next_state, reward, done in P[s][a]:\n",
    "\n",
    "                            # Calculate the expected value using equation 4.6\n",
    "                            v += action_prob * prob * (reward + gamma * ) \n",
    "                            \n",
    "                    # ------------ Deviation from 4.1 algorithm ------------ #\n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Iteration for Deterministic Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
