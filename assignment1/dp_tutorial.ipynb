{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> \n",
    "# Dynamic Programming Tutorial: Policy Iteration\n",
    "</font> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "The purpose of this tutorial is to walk-through how to implement Value Iteration, a dynamic programming method. We will use frozenlake, an existing environment developed by openai gym. This tutotial will be broken up into four parts (see below). We will also walk-through the deterministic and stochastic cases and a few discount factors to gain intuition for how this algorithm works.  \n",
    "\n",
    "### 4 Parts:\n",
    "\n",
    "- Policy Evaluation (prediction)\n",
    "- Policy Improvement\n",
    "- Policy Iteration \n",
    "- Value Iteration\n",
    "\n",
    "Reference: Sutton & Barto, 2018, Reinforcement Learning: An Introduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment \n",
    "\n",
    "Reference: https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "\n",
    "\n",
    "__Environment__\n",
    "\n",
    "A 4x4 gridworld with 4 state types. When you render the environment, the gridworld will be represented as: \n",
    "\n",
    "SFFF       \n",
    "FHFH       \n",
    "FFFH       \n",
    "HFFG  \n",
    "\n",
    "where\n",
    "\n",
    "- S: starting point, safe\n",
    "- F: frozen surface, safe\n",
    "- H: hole, fall to your doom\n",
    "- G: goal, where the frisbee is located\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "__Actions__\n",
    "- a == 0 is Left \n",
    "- a == 1 is Down\n",
    "- a == 2 is Right\n",
    "- a == 3 is Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from lake_envs import *\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering Function -- Do NOT need to modify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_single(env, policy, max_steps=100):\n",
    "  \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "  \"\"\"\n",
    "\n",
    "  episode_reward = 0\n",
    "  ob = env.reset()\n",
    "  for t in range(max_steps):\n",
    "    env.render()\n",
    "    time.sleep(0.25)\n",
    "    a = policy[ob]\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    episode_reward += rew\n",
    "    if done:\n",
    "      break\n",
    "  env.render();\n",
    "  if not done:\n",
    "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "  else:\n",
    "    print('# Max Steps: ',max_steps)\n",
    "    print(\"Episode reward: %f\" % episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine & Initialize Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability, nextstate, reward, terminal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 0, 0.0, False)],\n",
       " 1: [(1.0, 4, 0.0, False)],\n",
       " 2: [(1.0, 1, 0.0, False)],\n",
       " 3: [(1.0, 0, 0.0, False)]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the deterministic environment\n",
    "env_d = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "print('probability, nextstate, reward, terminal')\n",
    "env_d.P[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability, nextstate, reward, terminal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False)],\n",
       " 2: [(0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the stochastic environment\n",
    "env_s = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "print('probability, nextstate, reward, terminal')\n",
    "env_s.P[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Understanding\n",
    "\n",
    "Notice that the deterministic environment has 1 cell per parameter, but the stochastic environment has 3 cells. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> \n",
    "## Policy Evaluation (prediction): \n",
    "</font>\n",
    "\n",
    "The process for evaluating a policy is to successively approximate and update the value of a state using the Bellman equation. The old state's value is replaced with a new value. The new value is obtained by using the old value of the successor state, s', and the rewards we expect to get at the next state. Then, the value function is computer by summing the expectations for all of the successeeding next states.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm: Iterative Policy Evaluation for estimating V~v_pi [p. 75]\n",
    "\n",
    "Input pi, the policy we are evaluating\n",
    "\n",
    "Set theta > 0, a small threshold that will determine the accuracy of our policy's estimation\n",
    "\n",
    "Initialize V(s) arbitrarily, the initial state values, for all states except the terminal state set to zero \n",
    "\n",
    "Loop:\n",
    "\n",
    "    delta <- 0\n",
    "    Loop for each state in S:\n",
    "        v <- V(s)\n",
    "        V(s) <- sum over all actions, pi(a|s) \n",
    "                * sum over all next states and their corresponding \n",
    "                rewards, p(s',r|s,a)[r + gamma * V(s')]\n",
    "        delta <- max(delta, |v - V(s)|) \n",
    "    until delta < theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=1, tol=1e-3, run_num_episodes=5):\n",
    "        \n",
    "    # Initialize value function & delta\n",
    "    value_function = np.zeros(nS)\n",
    "    delta = np.inf\n",
    "    episode = 0\n",
    "    \n",
    "    print('Env. Action Probability:',P[0][0])\n",
    "    print('Initial Value Function',value_function)\n",
    "    print('Policy',policy)\n",
    "    \n",
    "    # Policy eval. will terminate when the value function's change is below the threshold   \n",
    "    while episode < run_num_episodes:\n",
    "    # while delta >= tol:    \n",
    "        \n",
    "        print('\\nEpisode:',episode, ' &  Value Function',value_function)\n",
    "        \n",
    "        '''\n",
    "        prev_value_function = value_function.copy()\n",
    "        for state in range(nS):\n",
    "            value_function[state] = 0\n",
    "            action = policy[state] #policy is deterministic\n",
    "            for next_tuple in range(len(P[state][action])):\n",
    "                prob = P[state][action][next_tuple][0]\n",
    "                nextstate = P[state][action][next_tuple][1]\n",
    "                reward = P[state][action][next_tuple][2]\n",
    "                terminal = P[state][action][next_tuple][3]\n",
    "                #print(reward)\n",
    "                value_function[state] += prob*(reward + gamma * (1-terminal)*value_function[nextstate])\n",
    "        delta = np.amax(np.abs(value_function - prev_value_function))\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        # Why do we loop through all (16) states? \n",
    "        for s in range(nS):   \n",
    "            v = value_function[s]\n",
    "            a = policy[s]\n",
    "            value_function[s] = 0\n",
    "            \n",
    "            for parameter in range(len(P[s][a])):\n",
    "            \n",
    "                prob = P[s][a][parameter][0]  \n",
    "                nextstate = P[s][a][parameter][1]\n",
    "                reward = P[s][a][parameter][2]\n",
    "                done = P[s][a][parameter][3]\n",
    "                \n",
    "                # Print out P[s][a] as it's useful for debugging.\n",
    "                print('prob, state, reward, done:',P[s][a])\n",
    "                #print('value nextstate:',value_function[nextstate])\n",
    "                \n",
    "                value_function = prob * (reward + gamma * value_function[nextstate])\n",
    "                print('value[',s,']:',value_function[s])\n",
    "                                       \n",
    "                # Compute the change in value functions across states\n",
    "                delta = max(delta, np.abs(v - value_function[s]))\n",
    "        '''\n",
    "        \n",
    "        episode += 1\n",
    "        \n",
    "    # Final value function\n",
    "    print('Final # of Episodes: ',episode)\n",
    "    \n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>Evaluate Deterministic Policies</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine a Deterministic Policies for a Discount Value of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Deterministic Zeros Policy\n",
    "policy = np.zeros(env_d.nS, dtype=int)\n",
    "print('Policy: ',policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Zero Policy Iteration\n",
      "-------------------------------\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  3\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a Deterministic Zeros Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Zero Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=1, tol=1e-3, run_num_episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Values:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Examine a Deterministic Zeros Policy & Values\n",
    "print('Policy: ',policy)\n",
    "print('Values: ',state_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Understanding -- Zero Policy\n",
    "\n",
    "Does this value function make sense to you? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 3 steps.\n"
     ]
    }
   ],
   "source": [
    "# Inspect Behavior for a Deterministic Zeros Policy\n",
    "render_single(env_d, policy, max_steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Understanding -- Zero Policy\n",
    "\n",
    "Our agent doesn't reach a terminal state in 5 steps. It turns out that if we instead tried for 100 steps, our agent still wouldn't reach a terminal state. In an environment of only 16 states and 64 actions, it seems like we should reach a terminal states, so why don't we? (Hint: Observe the behavior of the highlighted state.)\n",
    "\n",
    "If the values of our value function didn't make sense to you before, do they now? What new realization did you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Ones Policy Iteration\n",
      "-------------------------------\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  3\n",
      "\n",
      "Policy:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Final Values:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "# Max Steps:  3\n",
      "Episode reward: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Ones Policy\n",
    "policy = np.ones(env_d.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Ones Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Ones Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=1, tol=1e-3, run_num_episodes=3)\n",
    "\n",
    "# Examine a Ones Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Final Values: ',state_values)\n",
    "\n",
    "# Inspect the Behavior of a Ones Policy\n",
    "render_single(env_d, policy, max_steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Understanding -- Ones Policy\n",
    "\n",
    "This policy does reach a terminal state after only 5 steps. Additionally, there is a difference in behavior with \n",
    "a policy of ones. What is that behavior? What does this mean? (Hint: If you're unsure test for policies of all twos, threes, fours and so on until you've figured it out.)\n",
    "\n",
    "For a twos policy, the agent would move to the right and get stuck after reaching state 3, while in the case of a threes policy it would get stuck in trying to move up and thus would never leave the starting state. This means that you will not see any value changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Deterministic Policies for a  <font color=blue>Discount Value of 0.9</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Zero Policy Iteration\n",
      "-------------------------------\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  3\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Deterministic Zeros Policy\n",
    "policy = np.zeros(env_d.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Deterministic Zeros Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Zero Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=0.9, tol=1e-3, run_num_episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Final Values:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 3 steps.\n"
     ]
    }
   ],
   "source": [
    "# Examine a Deterministic Zeros Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Final Values: ',state_values)\n",
    "\n",
    "# Inspect Behavior for a Deterministic Zeros Policy\n",
    "render_single(env_d, policy, max_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Twos Policy Iteration\n",
      "-------------------------------\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Deterministic Twos Policy\n",
    "policy = np.array([2]*env_d.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Determinstic Twos Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Twos Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=0.9, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy:  [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Values:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 5 steps.\n"
     ]
    }
   ],
   "source": [
    "# Examine a Deterministic Twos Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Values: ',state_values)\n",
    "\n",
    "# Inspect Behavior for a Deterministic Twos Policy\n",
    "render_single(env_d, policy, max_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding \n",
    "\n",
    "Why don't we reach a terminal state in this case?\n",
    "\n",
    "For all of these policies we have the same issue, the same action at every state doesn't lead the agent to the goal state. So what can we do? Let's be random!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate a Deterministic  <font color=blue>Random </font> Policy for Discount Factor 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy:  [2 1 2 3 2 2 3 1 3 1 2 1 0 2 1 3]\n"
     ]
    }
   ],
   "source": [
    "rpolicy = np.random.choice(env_d.nA, env_d.nS)\n",
    "print('\\nPolicy: ',rpolicy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although randomness might increase our chances of getting somewhere closer to the goal state, it still doesn't produce the goal driven behavior. Knowing that moving into the goal state provide the agent with reward, let's add an action that will produce reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Random Policy Iteration\n",
      "-------------------------------\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [2 1 2 3 2 2 3 1 3 1 2 1 0 2 1 3]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "\n",
      "Policy:  [2 1 2 3 2 2 3 1 3 1 2 1 0 2 1 3]\n",
      "Values:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a Deterministic Random Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Random Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, rpolicy, gamma=0.9, tol=1e-3, run_num_episodes=5)\n",
    "\n",
    "# Examine a Deterministic Random Policy & Values\n",
    "print('\\nPolicy: ',rpolicy)\n",
    "print('Values: ',state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force the agent to move to the right \n",
    "rpolicy[14] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Random Policy Iteration\n",
      "-------------------------------\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [2 1 2 3 2 2 3 1 3 1 2 1 0 2 2 3]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  3\n",
      "\n",
      "Policy:  [2 1 2 3 2 2 3 1 3 1 2 1 0 2 2 3]\n",
      "Values:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a Deterministic Random Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Random Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, rpolicy, gamma=0.9, tol=1e-3, run_num_episodes=3)\n",
    "\n",
    "# Examine a Deterministic Random Policy & Values\n",
    "print('\\nPolicy: ',rpolicy)\n",
    "print('Values: ',state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "# Max Steps:  10\n",
      "Episode reward: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Inspect Behavior for a Deterministic Twos Policy\n",
    "render_single(env_d, rpolicy, max_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Understanding \n",
    "\n",
    "Does rigging the policy impact our value function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>Evaluate Stochastic Policies </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Stochastic Policies for Discount Value of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall we initialized a stochastic environment\n",
    "env_s = gym.make(\"Stochastic-4x4-FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding\n",
    "\n",
    "As before, we could evaluate and examine what happens for a zero, one ...etc. stochastic policies, but you should be able to say. What is that behavior? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Twos Policy Iteration\n",
      "-------------------------------\n",
      "Env. Action Probability: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Stochastic Twos Policy\n",
    "policy = np.array([2]*env_s.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Stochastic Twos Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Twos Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_s.P, env_s.nS, env_s.nA, policy, gamma=1, tol=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy:  [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Values:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "# Max Steps:  10\n",
      "Episode reward: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Examine a Stochastic Twos Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Values: ',state_values)\n",
    "\n",
    "# Inspect Behavior for a Stochastic Twos Policy\n",
    "render_single(env_s, policy, max_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Stochastic Policies for Discount Value of 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Twos Policy Iteration\n",
      "-------------------------------\n",
      "Env. Action Probability: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "\n",
      "Policy:  [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Values:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Stochastic Twos Policy\n",
    "policy = np.array([2]*env_s.nS, dtype=int)\n",
    "\n",
    "# Evaluate a Stochastic Twos Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Twos Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_s.P, env_s.nS, env_s.nA, policy, gamma=0.9, tol=1e-3)\n",
    "\n",
    "# Examine a Stochastic Twos Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Values: ',state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 3 steps.\n"
     ]
    }
   ],
   "source": [
    "# Inspect Behavior for a Stochastic Twos Policy\n",
    "render_single(env_s, policy, max_steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Stochastic Random Policies for Discount Value 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------\n",
      "Beginning Stochastic Random Policy Iteration\n",
      "---------------------------------------------\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 3 0 2 2 1 0 0 2 3 3 1 3 2]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n"
     ]
    }
   ],
   "source": [
    "# Initialize Policy \n",
    "rpolicy = np.random.choice(env_d.nA, env_d.nS)\n",
    "\n",
    "# Evaluate a Deterministic Random Policy \n",
    "print(\"\\n\" + \"-\"*45 + \"\\nBeginning Stochastic Random Policy Iteration\\n\" + \"-\"*45)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, rpolicy, gamma=0.9, tol=1e-3, run_num_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy:  [0 0 0 3 0 2 2 1 0 0 2 3 3 1 3 2]\n",
      "Values:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 3 steps.\n"
     ]
    }
   ],
   "source": [
    "# Examine a Deterministic Random Policy & Values\n",
    "print('\\nPolicy: ',rpolicy)\n",
    "print('Values: ',state_values)\n",
    "\n",
    "# Inspect Behavior for a Stochastic Twos Policy\n",
    "render_single(env_s, rpolicy, max_steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> \n",
    "# Policy Iteration \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration (using iterative policy evaluation) for estimating pi ~= pi*\n",
    "\n",
    "1. Initialization \n",
    "V(s) is an element of the real numbers for all states\n",
    "pi(s) is an element of A(s) for all states \n",
    "\n",
    "2. Policy Evaluation (implemented above)\n",
    "Loop: \n",
    "    delta <- 0\n",
    "    Loop for each s in the set of States:\n",
    "        v <- V(s)\n",
    "        V(s) <- sum over all next states & rewards p(s',r|s,a) * [r + gamma * V(s')]\n",
    "        delta <- max(delta, |v-V(s)|)\n",
    "    until delta < theta \n",
    "    \n",
    "    \n",
    "3. Policy Improvement (still need to implement)\n",
    "\n",
    "policy-stable <- true\n",
    "\n",
    "For each s in the set of States:\n",
    "\n",
    "    old-action <- pi(s)   \n",
    "    pi(s) <- argmax over actions sum over all next states and rewards, p(s',r|s,a)[r + gamma * V(s)] \n",
    "    If old-action =/= pi(s), then policy-stable <- false \n",
    "    \n",
    "If policy-stable, then stop and return V~=~v* & pi~=~pi*; else go to 2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=1):\n",
    "   \n",
    "    new_policy = np.zeros(nS, dtype='int')\n",
    "\n",
    "    action_values = np.zeros(nA)\n",
    "    for s in range(nS):\n",
    "        for a in range(nA):\n",
    "            for parameter in range(len(P[s][a])):\n",
    "            \n",
    "                prob = P[s][a][parameter][0]  \n",
    "                nextstate = P[s][a][parameter][1]\n",
    "                reward = P[s][a][parameter][2]\n",
    "                done = P[s][a][parameter][3]\n",
    "                \n",
    "            print('prob, state, reward, done:',P[s][a])\n",
    "            #print('value nextstate:',value_function[nextstate])\n",
    "            \n",
    "            action_values[a] += prob * (reward + (1-done) * gamma * value_from_policy[nextstate])\n",
    "            \n",
    "        new_policy[s] = np.argmax(action_values)       \n",
    "        \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=10e-3):\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "\n",
    "    loop = 0 \n",
    "    policy_stable = False\n",
    "    #while not policy_stable:\n",
    "    while loop < 10:\n",
    "        \n",
    "        # Policy Evaluation\n",
    "        value_from_policy = policy_evaluation(P, nS, nA, policy, gamma=gamma, tol=tol)\n",
    "        \n",
    "        # Policy Improvement\n",
    "        new_policy = policy_improvement(P, nS, nA, value_from_policy, policy, gamma=gamma)\n",
    "        \n",
    "        loop += 1 \n",
    "        print('Loop:',loop)\n",
    "        \n",
    "        if (policy == new_policy).all():\n",
    "            policy_stable = True\n",
    "            \n",
    "        policy = new_policy.copy()\n",
    "        value_function = value_from_policy.copy()\n",
    "\n",
    "    return value_function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & Improve a <font color=blue>Deterministic</font> Random Policy with Discount Factor 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [3 3 1 0 3 1 3 0 2 3 2 2 1 3 3 3]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 5  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 6  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 7  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 8  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 9  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  10\n"
     ]
    }
   ],
   "source": [
    "# Initialize a random policy \n",
    "policy = np.random.choice(env_d.nA, env_d.nS)\n",
    "\n",
    "# Evaluate your policy's value function\n",
    "value_from_policy = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=0.9, tol=1e-3, run_num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deterministic Policy: [3 3 1 0 3 1 3 0 2 3 2 2 1 3 3 3]\n",
      "Value Function: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('Deterministic Policy:',policy)\n",
    "print('Value Function:', value_from_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_improvement(env_d.P, env_d.nS, env_d.nA, state_values, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Policy Iteration\n",
      "-------------------------\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 1\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 2\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 3\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 4\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 5\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 6\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 7\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 8\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 9\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(1.0, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "# Evaluate & Improve Deterministic Random Policy\n",
    "V_pi, p_pi = policy_iteration(env_d.P, env_d.nS, env_d.nA, gamma=0.9, tol=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 100 steps.\n"
     ]
    }
   ],
   "source": [
    "# Inspect Behavior \n",
    "render_single(env_d, p_pi, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and Improve a <font color=blue> Stochastic</font> Random Policy with Discount Factor 0.9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Policy Iteration\n",
      "-------------------------\n",
      "Env. Action Probability: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 1\n",
      "Env. Action Probability: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 2\n",
      "Env. Action Probability: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 3\n",
      "Env. Action Probability: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 4\n",
      "Env. Action Probability: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 5\n",
      "Env. Action Probability: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 6\n",
      "Env. Action Probability: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 7\n",
      "Env. Action Probability: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 8\n",
      "Env. Action Probability: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 9\n",
      "Env. Action Probability: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Final # of Episodes:  5\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 5, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 7, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 11, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 12, 0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "prob, state, reward, done: [(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "prob, state, reward, done: [(1.0, 15, 0, True)]\n",
      "Loop: 10\n"
     ]
    }
   ],
   "source": [
    "# Initialize a stochastic random policy \n",
    "policy = np.random.choice(env_s.nA, env_s.nS)\n",
    "policy[14] = 2\n",
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "# Evaluate & Improve Deterministic Random Policy\n",
    "V_pi, p_pi = policy_iteration(env_s.P, env_s.nS, env_s.nA, gamma=0.9, tol=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
