{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from lake_envs import *\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_single(env, policy, max_steps=100):\n",
    "  \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "  \"\"\"\n",
    "\n",
    "  episode_reward = 0\n",
    "  ob = env.reset()\n",
    "  for t in range(max_steps):\n",
    "    env.render()\n",
    "    time.sleep(0.25)\n",
    "    a = policy[ob]\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    episode_reward += rew\n",
    "    if done:\n",
    "      break\n",
    "  env.render();\n",
    "  if not done:\n",
    "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "  else:\n",
    "    print('# Max Steps: ',max_steps)\n",
    "    print(\"Episode reward: %f\" % episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability, nextstate, reward, terminal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 0, 0.0, False)],\n",
       " 1: [(1.0, 4, 0.0, False)],\n",
       " 2: [(1.0, 1, 0.0, False)],\n",
       " 3: [(1.0, 0, 0.0, False)]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the deterministic environment\n",
    "env_d = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "print('probability, nextstate, reward, terminal')\n",
    "env_d.P[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability, nextstate, reward, terminal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False)],\n",
       " 2: [(0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the stochastic environment\n",
    "env_s = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "print('probability, nextstate, reward, terminal')\n",
    "env_s.P[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=1, tol=1e-3, run_num_episodes=5):\n",
    "        \n",
    "    # Initialize value function & delta\n",
    "    value_function = np.zeros(nS)\n",
    "    delta = np.inf\n",
    "    episode = 0\n",
    "    \n",
    "    print('SANITY CHECK')\n",
    "    print('Env. Action Probability:',P[0][0])\n",
    "    print('Initial Value Function',value_function)\n",
    "    print('Policy',policy)\n",
    "    \n",
    "    # Policy eval. will terminate when the value function's change is below the threshold   \n",
    "    while episode < run_num_episodes:\n",
    "    # while delta >= tol:    \n",
    "        \n",
    "        print('\\nEpisode:',episode, ' &  Value Function',value_function)\n",
    "        \n",
    "        # Why do we loop through all (16) states? \n",
    "        for s in range(nS):   \n",
    "            v = value_function[s]\n",
    "            a = policy[s]\n",
    "            \n",
    "            for parameter in range(len(P[s][a])):\n",
    "            \n",
    "                prob = P[s][a][parameter][0]  \n",
    "                nextstate = P[s][a][parameter][1]\n",
    "                reward = P[s][a][parameter][2]\n",
    "                done = P[s][a][parameter][3]\n",
    "                \n",
    "                print('\\nstate:',s)\n",
    "                print('action:',policy[s])\n",
    "                print('P:',P[s][a])\n",
    "                print('prob:',prob)\n",
    "                print('value nextstate:',value_function[nextstate])\n",
    "                print('reward:',reward)\n",
    "                print('done:',done)\n",
    "                \n",
    "                state_value = prob * (reward + gamma * value_function[nextstate])\n",
    "                print('State Value:',state_value)\n",
    "                \n",
    "                value_function[s] += prob * (reward + gamma * value_function[nextstate]) \n",
    "                print('value_function[s]:',value_function[s])\n",
    "                                       \n",
    "                # Compute the change in value functions across states\n",
    "                delta = max(delta, np.abs(v - value_function[s]))\n",
    "\n",
    "        episode += 1\n",
    "        \n",
    "        \"\"\"\n",
    "        if episode % 10 == 0:\n",
    "            print('Episode: ',episode)\n",
    "            print('value_function: ',V)\n",
    "        \"\"\"\n",
    "        \n",
    "    # Final value function\n",
    "    print('Final # of Episodes: ',episode)\n",
    "        \n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Deterministic Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy:  [0 0 2 1 2 0 3 3 1 0 1 2 3 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "rpolicy = np.random.choice(env_d.nA, env_d.nS)\n",
    "rpolicy[14] = 2\n",
    "print('\\nPolicy: ',rpolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Random Policy Iteration\n",
      "-------------------------------\n",
      "SANITY CHECK\n",
      "Env. Action Probability: [(1.0, 0, 0.0, False)]\n",
      "Initial Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Policy [0 0 2 1 2 0 3 3 1 0 1 2 3 1 2 0]\n",
      "\n",
      "Episode: 0  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "state: 0\n",
      "action: 0\n",
      "P: [(1.0, 0, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 1\n",
      "action: 0\n",
      "P: [(1.0, 0, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 2\n",
      "action: 2\n",
      "P: [(1.0, 3, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 3\n",
      "action: 1\n",
      "P: [(1.0, 7, 0.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 4\n",
      "action: 2\n",
      "P: [(1.0, 5, 0.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 5\n",
      "action: 0\n",
      "P: [(1.0, 5, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 6\n",
      "action: 3\n",
      "P: [(1.0, 2, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 7\n",
      "action: 3\n",
      "P: [(1.0, 7, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 8\n",
      "action: 1\n",
      "P: [(1.0, 12, 0.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 9\n",
      "action: 0\n",
      "P: [(1.0, 8, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 10\n",
      "action: 1\n",
      "P: [(1.0, 14, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 11\n",
      "action: 2\n",
      "P: [(1.0, 11, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 12\n",
      "action: 3\n",
      "P: [(1.0, 12, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 13\n",
      "action: 1\n",
      "P: [(1.0, 13, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 14\n",
      "action: 2\n",
      "P: [(1.0, 15, 1.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 1.0\n",
      "done: True\n",
      "State Value: 1.0\n",
      "value_function[s]: 1.0\n",
      "\n",
      "state: 15\n",
      "action: 0\n",
      "P: [(1.0, 15, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "Episode: 1  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "state: 0\n",
      "action: 0\n",
      "P: [(1.0, 0, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 1\n",
      "action: 0\n",
      "P: [(1.0, 0, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 2\n",
      "action: 2\n",
      "P: [(1.0, 3, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 3\n",
      "action: 1\n",
      "P: [(1.0, 7, 0.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 4\n",
      "action: 2\n",
      "P: [(1.0, 5, 0.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 5\n",
      "action: 0\n",
      "P: [(1.0, 5, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 6\n",
      "action: 3\n",
      "P: [(1.0, 2, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 7\n",
      "action: 3\n",
      "P: [(1.0, 7, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 8\n",
      "action: 1\n",
      "P: [(1.0, 12, 0.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 9\n",
      "action: 0\n",
      "P: [(1.0, 8, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 10\n",
      "action: 1\n",
      "P: [(1.0, 14, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 1.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 1.0\n",
      "value_function[s]: 1.0\n",
      "\n",
      "state: 11\n",
      "action: 2\n",
      "P: [(1.0, 11, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 12\n",
      "action: 3\n",
      "P: [(1.0, 12, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 13\n",
      "action: 1\n",
      "P: [(1.0, 13, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 14\n",
      "action: 2\n",
      "P: [(1.0, 15, 1.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 1.0\n",
      "done: True\n",
      "State Value: 1.0\n",
      "value_function[s]: 2.0\n",
      "\n",
      "state: 15\n",
      "action: 0\n",
      "P: [(1.0, 15, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "Episode: 2  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 2. 0.]\n",
      "\n",
      "state: 0\n",
      "action: 0\n",
      "P: [(1.0, 0, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 1\n",
      "action: 0\n",
      "P: [(1.0, 0, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 2\n",
      "action: 2\n",
      "P: [(1.0, 3, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 3\n",
      "action: 1\n",
      "P: [(1.0, 7, 0.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 4\n",
      "action: 2\n",
      "P: [(1.0, 5, 0.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 5\n",
      "action: 0\n",
      "P: [(1.0, 5, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 6\n",
      "action: 3\n",
      "P: [(1.0, 2, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 7\n",
      "action: 3\n",
      "P: [(1.0, 7, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 8\n",
      "action: 1\n",
      "P: [(1.0, 12, 0.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 9\n",
      "action: 0\n",
      "P: [(1.0, 8, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 10\n",
      "action: 1\n",
      "P: [(1.0, 14, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 2.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 2.0\n",
      "value_function[s]: 3.0\n",
      "\n",
      "state: 11\n",
      "action: 2\n",
      "P: [(1.0, 11, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 12\n",
      "action: 3\n",
      "P: [(1.0, 12, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 13\n",
      "action: 1\n",
      "P: [(1.0, 13, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 14\n",
      "action: 2\n",
      "P: [(1.0, 15, 1.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 1.0\n",
      "done: True\n",
      "State Value: 1.0\n",
      "value_function[s]: 3.0\n",
      "\n",
      "state: 15\n",
      "action: 0\n",
      "P: [(1.0, 15, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "Episode: 3  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 3. 0.]\n",
      "\n",
      "state: 0\n",
      "action: 0\n",
      "P: [(1.0, 0, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 1\n",
      "action: 0\n",
      "P: [(1.0, 0, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 2\n",
      "action: 2\n",
      "P: [(1.0, 3, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 3\n",
      "action: 1\n",
      "P: [(1.0, 7, 0.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 4\n",
      "action: 2\n",
      "P: [(1.0, 5, 0.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 5\n",
      "action: 0\n",
      "P: [(1.0, 5, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 6\n",
      "action: 3\n",
      "P: [(1.0, 2, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 7\n",
      "action: 3\n",
      "P: [(1.0, 7, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 8\n",
      "action: 1\n",
      "P: [(1.0, 12, 0.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 9\n",
      "action: 0\n",
      "P: [(1.0, 8, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 10\n",
      "action: 1\n",
      "P: [(1.0, 14, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 3.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 3.0\n",
      "value_function[s]: 6.0\n",
      "\n",
      "state: 11\n",
      "action: 2\n",
      "P: [(1.0, 11, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 12\n",
      "action: 3\n",
      "P: [(1.0, 12, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 13\n",
      "action: 1\n",
      "P: [(1.0, 13, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 14\n",
      "action: 2\n",
      "P: [(1.0, 15, 1.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 1.0\n",
      "done: True\n",
      "State Value: 1.0\n",
      "value_function[s]: 4.0\n",
      "\n",
      "state: 15\n",
      "action: 0\n",
      "P: [(1.0, 15, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "Episode: 4  &  Value Function [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 6. 0. 0. 0. 4. 0.]\n",
      "\n",
      "state: 0\n",
      "action: 0\n",
      "P: [(1.0, 0, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 1\n",
      "action: 0\n",
      "P: [(1.0, 0, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 2\n",
      "action: 2\n",
      "P: [(1.0, 3, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 3\n",
      "action: 1\n",
      "P: [(1.0, 7, 0.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 4\n",
      "action: 2\n",
      "P: [(1.0, 5, 0.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 5\n",
      "action: 0\n",
      "P: [(1.0, 5, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 6\n",
      "action: 3\n",
      "P: [(1.0, 2, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 7\n",
      "action: 3\n",
      "P: [(1.0, 7, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 8\n",
      "action: 1\n",
      "P: [(1.0, 12, 0.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 9\n",
      "action: 0\n",
      "P: [(1.0, 8, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 10\n",
      "action: 1\n",
      "P: [(1.0, 14, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 4.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 4.0\n",
      "value_function[s]: 10.0\n",
      "\n",
      "state: 11\n",
      "action: 2\n",
      "P: [(1.0, 11, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 12\n",
      "action: 3\n",
      "P: [(1.0, 12, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 13\n",
      "action: 1\n",
      "P: [(1.0, 13, 0.0, False)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0.0\n",
      "done: False\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "\n",
      "state: 14\n",
      "action: 2\n",
      "P: [(1.0, 15, 1.0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 1.0\n",
      "done: True\n",
      "State Value: 1.0\n",
      "value_function[s]: 5.0\n",
      "\n",
      "state: 15\n",
      "action: 0\n",
      "P: [(1.0, 15, 0, True)]\n",
      "prob: 1.0\n",
      "value nextstate: 0.0\n",
      "reward: 0\n",
      "done: True\n",
      "State Value: 0.0\n",
      "value_function[s]: 0.0\n",
      "Final # of Episodes:  5\n",
      "\n",
      "Policy:  [0 0 2 1 2 0 3 3 1 0 1 2 3 1 2 0]\n",
      "Values:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10.  0.  0.  0.  5.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a Stochastic Zeros Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Random Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, rpolicy, gamma=1, tol=1e-3)\n",
    "\n",
    "# Examine a Stochastic Zeros Policy & Values\n",
    "print('\\nPolicy: ',rpolicy)\n",
    "print('Values: ',state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-00c3be987846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrender_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'policy' is not defined"
     ]
    }
   ],
   "source": [
    "render_single(env_d, policy, max_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Stochastic Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy:  [1 3 3 3 0 3 3 0 0 2 1 3 1 2 3 3]\n"
     ]
    }
   ],
   "source": [
    "policy = np.random.choice(env_s.nA, env_s.nS)\n",
    "print('\\nPolicy: ',policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a Stochastic Zeros Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Zeros Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_s.P, env_s.nS, env_s.nA, policy, gamma=1, tol=1e-3)\n",
    "\n",
    "# Examine a Stochastic Zeros Policy & Values\n",
    "print('\\nPolicy: ',policy)\n",
    "print('Values: ',state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Behavior for a Stochastic Zeros Policy\n",
    "render_single(env_s, policy, max_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                    # ------------ Deviation from 4.1 algorithm ------------ #\\n                    # Loop through set of possible next actions\\n                    \\n                    for a, action_prob in enumerate(policy[s]):\\n                        # For each action, look at its possible next state\\n                        for prob, next_state, reward, done in P[s][a]:\\n\\n                            # Calculate the expected value using equation 4.6\\n                            v += action_prob * prob * (reward + gamma * ) \\n                            \\n                    # ------------ Deviation from 4.1 algorithm ------------ #\\n                    '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Policy Evaluation \n",
    "\"\"\"\n",
    "                    # ------------ Deviation from 4.1 algorithm ------------ #\n",
    "                    # Loop through set of possible next actions\n",
    "                    \n",
    "                    for a, action_prob in enumerate(policy[s]):\n",
    "                        # For each action, look at its possible next state\n",
    "                        for prob, next_state, reward, done in P[s][a]:\n",
    "\n",
    "                            # Calculate the expected value using equation 4.6\n",
    "                            v += action_prob * prob * (reward + gamma * ) \n",
    "                            \n",
    "                    # ------------ Deviation from 4.1 algorithm ------------ #\n",
    "                    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
