{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction \n",
    "\n",
    "The purpose of this tutorial is to walk-through how to implement Value Iteration, a dynamic programming method. We will use frozenlake, an existing environment developed by openai gym. This tutotial will be broken up into four parts (see below). We will also walk-through the deterministic and stochastic cases and a few discount factors to gain intuition for how this algorithm works.  \n",
    "\n",
    "#### 4 Parts:\n",
    "\n",
    "- Policy Evaluation (prediction)\n",
    "- Policy Improvement\n",
    "- Policy Iteration \n",
    "- Value Iteration\n",
    "\n",
    "Reference: Sutton & Barto, 2018, Reinforcement Learning: An Introduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment \n",
    "\n",
    "Reference: https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "A 4x4 gridworld with several states: \n",
    "\n",
    "- S: starting point, safe\n",
    "- F: frozen surface, safe\n",
    "- H: hole, fall to your doom\n",
    "- G: goal, where the frisbee is located\n",
    "\n",
    "When you run the environment, the gridworld will be represented as: \n",
    "\n",
    "SFFF       \n",
    "FHFH       \n",
    "FFFH       \n",
    "HFFG  \n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from lake_envs import *\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering Function -- Do NOT need to modify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_single(env, policy, max_steps=100):\n",
    "  \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "  \"\"\"\n",
    "\n",
    "  episode_reward = 0\n",
    "  ob = env.reset()\n",
    "  for t in range(max_steps):\n",
    "    env.render()\n",
    "    time.sleep(0.25)\n",
    "    a = policy[ob]\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    episode_reward += rew\n",
    "    if done:\n",
    "      break\n",
    "  env.render();\n",
    "  if not done:\n",
    "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "  else:\n",
    "  \tprint(\"Episode reward: %f\" % episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine & Initialize Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability, nextstate, reward, terminal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 0, 0.0, False)],\n",
       " 1: [(1.0, 4, 0.0, False)],\n",
       " 2: [(1.0, 1, 0.0, False)],\n",
       " 3: [(1.0, 0, 0.0, False)]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the deterministic environment\n",
    "env_d = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "print('probability, nextstate, reward, terminal')\n",
    "env_d.P[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability, nextstate, reward, terminal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False)],\n",
       " 2: [(0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the stochastic environment\n",
    "env_s = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "print('probability, nextstate, reward, terminal')\n",
    "env_s.P[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Understanding\n",
    "\n",
    "Notice that the deterministic environment has 1 cell per parameter, but the stochastic environment has 3 cells. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation (prediction): \n",
    "\n",
    "The process for evaluating a policy is to successively approximate and update the value of a state using the Bellman equation. The old state's value is replaced with a new value. The new value is obtained by using the old value of the successor state, s', and the rewards we expect to get at the next state. Then, the value function is computer by summing the expectations for all of the successeeding next states.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm: Iterative Policy Evaluation for estimating V~v_pi [p. 75]\n",
    "\n",
    "Input pi, the policy we are evaluating\n",
    "\n",
    "Set theta > 0, a small threshold that will determine the accuracy of our policy's estimation\n",
    "\n",
    "Initialize V(s) arbitrarily, the initial state values, for all states except the terminal state set to zero \n",
    "\n",
    "Loop:\n",
    "\n",
    "    delta <- 0\n",
    "    Loop for each state in S:\n",
    "        v <- V(s)\n",
    "        V(s) <- sum over all actions, pi(a|s) \n",
    "                * sum over all next states and their corresponding \n",
    "                rewards, p(s',r|s,a)[r + gamma * V(s')]\n",
    "        delta <- max(delta, |v - V(s)|) \n",
    "    until delta < theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=1, tol=1e-3):\n",
    "    \n",
    "    # Initialize value function & delta\n",
    "    V = np.zeros(nS)\n",
    "    delta = np.inf\n",
    "    episode = 0\n",
    "    \n",
    "    # Policy eval. will terminate when the value function's change is below the threshold\n",
    "    while episode < 10:\n",
    "    #while delta >= tol:\n",
    "        \n",
    "        print('Episode: ',episode)\n",
    "        print('Value Function: ',V)\n",
    "        \n",
    "        # Why do we loop through all the states? \n",
    "        for s in range(nS):   \n",
    "            v = V[s]\n",
    "            a = policy[s]\n",
    "            \n",
    "            for prob, nextstate, reward, done in P[s][a]:   \n",
    "                V[s] = prob * (reward + gamma * nextstate) \n",
    "                \n",
    "                #print('state:',s)\n",
    "                #print('action: ',a)\n",
    "                #print('prob: ',prob)\n",
    "                #print('nextstate: ',nextstate)\n",
    "                #print('reward: ',reward)\n",
    "                #print('done: ',done) \n",
    "                #print('value: ',V[s])\n",
    "                      \n",
    "                # Compute the change in value functions across states\n",
    "                delta = max(delta, np.abs(v - V[s]))\n",
    "\n",
    "        episode+=1\n",
    "        #if episode < 10:\n",
    "            #print('Episode: ',episode)\n",
    "            #print('Working Value Fcn: ',V)\n",
    "        \n",
    "        \"\"\"\n",
    "        if episode % 10 == 0:\n",
    "            print('Episode: ',episode)\n",
    "            print('value_function: ',V)\n",
    "        \"\"\"\n",
    "        \n",
    "    # Final value function\n",
    "    value_function = np.array(V)\n",
    "        \n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine & Initialize Different Deterministic Policies for Discount Value of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Zeros Policy\n",
    "policy = np.zeros(env_d.nS, dtype=int)\n",
    "print('Policy: ',policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Zero Policy Iteration\n",
      "-------------------------------\n",
      "Episode:  0\n",
      "Value Function:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode:  1\n",
      "Value Function:  [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode:  2\n",
      "Value Function:  [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode:  3\n",
      "Value Function:  [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode:  4\n",
      "Value Function:  [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode:  5\n",
      "Value Function:  [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode:  6\n",
      "Value Function:  [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode:  7\n",
      "Value Function:  [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode:  8\n",
      "Value Function:  [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n",
      "Episode:  9\n",
      "Value Function:  [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Zeros Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Zero Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=0.9, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Values:  [ 0.   0.   0.9  1.8  3.6  4.5  4.5  6.3  7.2  7.2  8.1  9.9 10.8 10.8\n",
      " 11.7 13.5]\n"
     ]
    }
   ],
   "source": [
    "# Examine Policy & Values\n",
    "print('Policy: ',policy)\n",
    "print('Values: ',state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 5 steps.\n"
     ]
    }
   ],
   "source": [
    "# Inspect Behavior\n",
    "render_single(env_d, policy, max_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Understanding\n",
    "\n",
    "Our agent doesn't reach a terminal state in 5 steps. It turns out that if we instead tried for 100 steps, our agent still wouldn't reach a terminal state. In an environment of only 16 states and 64 actions, it seems like we should reach a terminal states, so why don't we? (Hint: Observe the behavior of the highlighted state.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ones Policy\n",
    "policy = np.ones(env_d.nS, dtype=int)\n",
    "print('Policy: ',policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Beginning Ones Policy Iteration\n",
      "-------------------------------\n",
      "Episode:  0\n",
      "Value Function:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode:  1\n",
      "Value Function:  [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode:  2\n",
      "Value Function:  [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode:  3\n",
      "Value Function:  [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode:  4\n",
      "Value Function:  [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode:  5\n",
      "Value Function:  [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode:  6\n",
      "Value Function:  [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode:  7\n",
      "Value Function:  [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode:  8\n",
      "Value Function:  [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n",
      "Episode:  9\n",
      "Value Function:  [ 3.6  4.5  5.4  6.3  7.2  4.5  9.   6.3 10.8 11.7 12.6  9.9 10.8 11.7\n",
      " 12.6 13.5]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Ones Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Ones Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=0.9, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Episode reward: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Inspect Behavior\n",
    "render_single(env_d, policy, max_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Understanding\n",
    "\n",
    "This policy does reach a terminal state after only 5 steps. Additionally, there is a difference in behavior with \n",
    "a policy of ones. What is that behavior? What does this mean? (Hint: If you're unsure test for policies of all twos, threes, fours and so on until you've figured it out.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "policy = np.array([2]*env_d.nS, dtype=int)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Twos Policy\n",
    "policy = np.zeros(env_d.nS, dtype=int)\n",
    "print('Policy: ',policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Ones Policy \n",
    "print(\"\\n\" + \"-\"*31 + \"\\nBeginning Ones Policy Iteration\\n\" + \"-\"*31)\n",
    "state_values = policy_evaluation(env_d.P, env_d.nS, env_d.nA, policy, gamma=0.9, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Behavior\n",
    "render_single(env_d, policy, max_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                    # ------------ Deviation from 4.1 algorithm ------------ #\\n                    # Loop through set of possible next actions\\n                    \\n                    for a, action_prob in enumerate(policy[s]):\\n                        # For each action, look at its possible next state\\n                        for prob, next_state, reward, done in P[s][a]:\\n\\n                            # Calculate the expected value using equation 4.6\\n                            v += action_prob * prob * (reward + gamma * ) \\n                            \\n                    # ------------ Deviation from 4.1 algorithm ------------ #\\n                    '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Policy Evaluation \n",
    "\"\"\"\n",
    "                    # ------------ Deviation from 4.1 algorithm ------------ #\n",
    "                    # Loop through set of possible next actions\n",
    "                    \n",
    "                    for a, action_prob in enumerate(policy[s]):\n",
    "                        # For each action, look at its possible next state\n",
    "                        for prob, next_state, reward, done in P[s][a]:\n",
    "\n",
    "                            # Calculate the expected value using equation 4.6\n",
    "                            v += action_prob * prob * (reward + gamma * ) \n",
    "                            \n",
    "                    # ------------ Deviation from 4.1 algorithm ------------ #\n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Iteration for Deterministic Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
